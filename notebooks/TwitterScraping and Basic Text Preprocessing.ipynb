{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import twitter\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### https://github.com/bear/python-twitter/blob/master/get_access_token.py\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from requests_oauthlib import OAuth1Session\n",
    "import webbrowser\n",
    "\n",
    "import sys\n",
    "\n",
    "if sys.version_info.major < 3:\n",
    "    input = raw_input\n",
    "\n",
    "REQUEST_TOKEN_URL = 'https://api.twitter.com/oauth/request_token'\n",
    "ACCESS_TOKEN_URL = 'https://api.twitter.com/oauth/access_token'\n",
    "AUTHORIZATION_URL = 'https://api.twitter.com/oauth/authorize'\n",
    "SIGNIN_URL = 'https://api.twitter.com/oauth/authenticate'\n",
    "\n",
    "\n",
    "def get_access_token(consumer_key, consumer_secret):\n",
    "    \"\"\"Get an access token for a given consumer key and secret.\n",
    "    Args:\n",
    "        consumer_key (str):\n",
    "            Your application consumer key.\n",
    "        consumer_secret (str):\n",
    "            Your application consumer secret.\n",
    "    Returns:\n",
    "        (None) Prints to command line.\n",
    "    \"\"\"\n",
    "    oauth_client = OAuth1Session(consumer_key, client_secret=consumer_secret, callback_uri='oob')\n",
    "\n",
    "    print('\\nRequesting temp token from Twitter...\\n')\n",
    "\n",
    "    resp = oauth_client.fetch_request_token(REQUEST_TOKEN_URL)\n",
    "\n",
    "    url = oauth_client.authorization_url(AUTHORIZATION_URL)\n",
    "\n",
    "    print('I will try to start a browser to visit the following Twitter page '\n",
    "          'if a browser will not start, copy the URL to your browser '\n",
    "          'and retrieve the pincode to be used '\n",
    "          'in the next step to obtaining an Authentication Token: \\n'\n",
    "          '\\n\\t{0}'.format(url))\n",
    "\n",
    "    webbrowser.open(url)\n",
    "    pincode = input('\\nEnter your pincode? ')\n",
    "\n",
    "    print('\\nGenerating and signing request for an access token...\\n')\n",
    "\n",
    "    oauth_client = OAuth1Session(consumer_key, client_secret=consumer_secret,\n",
    "                                 resource_owner_key=resp.get('oauth_token'),\n",
    "                                 resource_owner_secret=resp.get('oauth_token_secret'),\n",
    "                                 verifier=pincode)\n",
    "    try:\n",
    "        resp = oauth_client.fetch_access_token(ACCESS_TOKEN_URL)\n",
    "    except ValueError as e:\n",
    "        raise 'Invalid response from Twitter requesting temp token: {0}'.format(e)\n",
    "\n",
    "    print('''Your tokens/keys are as follows:\n",
    "        consumer_key         = {ck}\n",
    "        consumer_secret      = {cs}\n",
    "        access_token_key     = {atk}\n",
    "        access_token_secret  = {ats}'''.format(\n",
    "            ck=consumer_key,\n",
    "            cs=consumer_secret,\n",
    "            atk=resp.get('oauth_token'),\n",
    "            ats=resp.get('oauth_token_secret')))\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run script to get access token and secret for given app.\"\"\"\n",
    "    consumer_key = input('Enter your consumer key: ')\n",
    "    consumer_secret = input('Enter your consumer secret: ')\n",
    "    get_access_token(consumer_key, consumer_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = twitter.Api(consumer_key=consumer_key,\n",
    "                              consumer_secret=consumer_secret,\n",
    "                              access_token_key=access_token_key,\n",
    "                              access_token_secret=access_token_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"created_at\": \"Mon Jun 15 22:05:34 +0000 2020\", \"default_profile\": true, \"description\": \"Cosmologists with interest in Earth\", \"favourites_count\": 12, \"followers_count\": 1, \"friends_count\": 35, \"id\": 1272650783918448640, \"id_str\": \"1272650783918448640\", \"name\": \"Vanessa Boehm\", \"profile_background_color\": \"F5F8FA\", \"profile_banner_url\": \"https://pbs.twimg.com/profile_banners/1272650783918448640/1592260389\", \"profile_image_url\": \"http://pbs.twimg.com/profile_images/1272658299679150081/hcWVZGJh_normal.jpg\", \"profile_image_url_https\": \"https://pbs.twimg.com/profile_images/1272658299679150081/hcWVZGJh_normal.jpg\", \"profile_link_color\": \"1DA1F2\", \"profile_sidebar_border_color\": \"C0DEED\", \"profile_sidebar_fill_color\": \"DDEEF6\", \"profile_text_color\": \"333333\", \"profile_use_background_image\": true, \"screen_name\": \"VanessaBoehm8\", \"status\": {\"created_at\": \"Tue Jun 16 21:56:41 +0000 2020\", \"id\": 1273011640758231041, \"id_str\": \"1273011640758231041\", \"lang\": \"en\", \"retweet_count\": 2, \"retweeted\": true, \"retweeted_status\": {\"created_at\": \"Mon Jun 15 20:50:25 +0000 2020\", \"favorite_count\": 3, \"id\": 1272632578822959104, \"id_str\": \"1272632578822959104\", \"lang\": \"en\", \"retweet_count\": 2, \"retweeted\": true, \"source\": \"<a href=\\\"https://mobile.twitter.com\\\" rel=\\\"nofollow\\\">Twitter Web App</a>\", \"text\": \"Housing advocates, tell San Ramon's Planning Commission to approve the CityWalk Master Plan by @bishopranch. This p\\u2026 https://t.co/WSvQlAUkp9\", \"truncated\": true}, \"source\": \"<a href=\\\"https://mobile.twitter.com\\\" rel=\\\"nofollow\\\">Twitter Web App</a>\", \"text\": \"RT @gbeltalliance: Housing advocates, tell San Ramon's Planning Commission to approve the CityWalk Master Plan by @bishopranch. This plan i\\u2026\"}, \"statuses_count\": 11}\n"
     ]
    }
   ],
   "source": [
    "print(api.VerifyCredentials())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[User(ID=1271253492603080705, ScreenName=chmod_i)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.GetFollowers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeline = api.GetUserTimeline(screen_name='BernieSanders', count=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'“My son and so many other children, their lives matter… My daughter’s life matters. So until we have change in our… https://t.co/IolQoxWJLS'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeline[0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_twitter_output(timeline):\n",
    "    output = []\n",
    "    for tweet in timeline:\n",
    "        tw ={'created':tweet.created_at,'text':tweet.text}\n",
    "        output.append(tw)\n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tweets = convert_twitter_output(timeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      “My son and so many other children, their live...\n",
       "1      This is what narcissism is about. Trump, in or...\n",
       "2      We are going to take the fight for justice fro...\n",
       "3      The people are crying out against police bruta...\n",
       "4      Congratulations to Rick Krajewski, Nikil Saval...\n",
       "                             ...                        \n",
       "195    This is catastrophic. We need to keep every em...\n",
       "196    The coronavirus crisis has had a devastating e...\n",
       "197    RT @RealJusticePAC: TONIGHT: Jails are major C...\n",
       "198    TONIGHT: Join us for a live town hall on the d...\n",
       "199    The climate crisis is the greatest challenge f...\n",
       "Name: text, Length: 200, dtype: object"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning \n",
    "\n",
    "def remove_url(txt):\n",
    "    return re.sub(r\"http\\S+\", \"\", txt)\n",
    "    \n",
    "def remove_rt(txt):\n",
    "    return re.sub(r\"RT\", \"\", txt)\n",
    "      \n",
    "def remove_hashtag(txt):\n",
    "    return re.sub(r'#\\S+','', txt)\n",
    "\n",
    "def remove_mentions(txt):\n",
    "    return re.sub(r'@\\S+','', txt)\n",
    "\n",
    "def remove_controls(txt):\n",
    "    return re.sub(r'[\\n\\r\\t\\f\\v]','',txt)\n",
    "\n",
    "def remove_whitespaces(txt):\n",
    "    return re.sub(r'  +','',txt)\n",
    "\n",
    "def remove_numbers(txt):\n",
    "    return re.sub(r'\\d','',txt)\n",
    "\n",
    "def remove_special_characters(txt):\n",
    "    return re.sub(r'[^\\w ]+','',txt)\n",
    "\n",
    "def remove_stopwords(txt):\n",
    "    for w\n",
    "\n",
    "function_list=[remove_url, remove_rt, remove_controls, remove_mentions, remove_hashtag, remove_whitespaces, remove_numbers, remove_special_characters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "for func in function_list:\n",
    "    df['text'] = df['text'].apply(lambda x: func(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "df['text'] = df['text'].apply(lambda x: tokenizer.tokenize(x.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords(x):\n",
    "    return [ww for ww in x if ww not in stopwords.words('english')]\n",
    "\n",
    "df['text'] = df['text'].apply(lambda x: remove_stopwords(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer= WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_lemmatizer(x):\n",
    "    return [lemmatizer.lemmatize(ww) for ww in x]\n",
    "\n",
    "df['text'] = df['text'].apply(lambda x: word_lemmatizer(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['narcissism',\n",
       " 'trump',\n",
       " 'order',\n",
       " 'hear',\n",
       " 'cheer',\n",
       " 'adoring',\n",
       " 'crowd',\n",
       " 'defy',\n",
       " 'science',\n",
       " 'sacri']"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TxtAnalysis",
   "language": "python",
   "name": "txtanalysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
